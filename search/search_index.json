{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/2024/01/23/accessibility-conflict/","title":"Accessibility Conflict","text":"<p>Automated test rack requires the injection of rotary input events using the <code>car_service</code> command, follow Set up a rotary controller.</p>"},{"location":"blog/2024/01/23/accessibility-conflict/#issue","title":"Issue","text":"<p>The <code>car_service</code> command can work normally:</p> adb shell cmd car_service inject-rotarySucceeded <p>However, when Appium is running, some commands to move focus do not work  (Commands that do not change focus still work), but the stdout is still <code>Succeeded</code>.</p> <p>After I force-stop <code>io.appium.uiautomator2.server</code>, all the commands work normally.  </p> Based on the phenomena, I guess there are two possibilities: <ul> <li>Appium process intercepts the rotary input events injection.</li> <li>The underlying service conflicts.</li> </ul>"},{"location":"blog/2024/01/23/accessibility-conflict/#solution","title":"Solution","text":"<p>Before this, I already knew that Appium were ultimately implemented based on Accessibility.  </p> <p>I've tried to learn how to implement Focus from Developing Apps without the Car UI Library.</p> <p>Luckily, I got important information in this doc, <code>FocusParkingView</code> and <code>FocusArea</code> are based on Accessibility too!</p> Quote <p>Implement FocusParkingView</p> <p>You either can implement your own <code>FocusParkingView</code> or copy the class from the car-ui-library to your project.</p> <p>To implement <code>FocusParkingView</code>:</p> <p>Hard code the accessibility class name so that the <code>RotaryService</code> can recognize it:</p> <pre><code>@Override\npublic CharSequence getAccessibilityClassName() {\n    return \"com.android.car.ui.FocusParkingView\";\n}\n</code></pre> <p>Implement FocusArea</p> <p>Like <code>FocusParkingView</code>, you can either implement your own <code>FocusArea</code> or copy the class from the car-ui-library to your project.</p> <p>To implement <code>FocusArea</code>:</p> <p>Hard code the accessibility class name so that rotary service can recognize it:</p> <pre><code>@Override\npublic CharSequence getAccessibilityClassName() {\n    return \"com.android.car.ui.FocusArea\";\n}\n</code></pre> <p>So, the root cause of this problem is Appium suppress Accessibility.  </p> <p>Knowing why, I found a solution in appium-uiautomator2-driver, set <code>appium:disableSuppressAccessibilityService</code> <code>true</code> when init driver.  </p> Quote Capability Name Description appium:disableSuppressAccessibilityService Being set to <code>true</code> tells the instrumentation process to not suppress accessibility services during the automated test. This might be useful if your automated test needs these services. <code>false</code> by default <p>Everything is ok now! \ud83c\udf89</p>"},{"location":"blog/2024/01/17/bfs-and-dfs-in-dict-traversal/","title":"BFS and DFS in Dict Traversal","text":"<p>I answered such a question on Stack Overflow:  How do I extract all keys from this JSON file?</p> <p>Retrieve all the dict keys from a dict whose values consist of multiple Iterable obj.</p> <pre><code>data = {\n    'k1': 'Lanbao',\n    'k2': {\n        'k3': 'Lanbao',\n        'k4': [\n            'Lanbao',\n            {\n                'k5': 'Lanbao',\n                'k6': ['Lanbao']\n            }\n        ],\n        'k7': 'Lanbao',\n    },\n    'k8': 'Lanbao'\n}\n</code></pre> <p>By constructing generators to implement BFS and DFS respectively.</p> bfs.py<pre><code>from collections import deque\nfrom collections.abc import Iterable\n\n\ndef get_keys(data, ignore_type=(str, bytes)):\n    deque_ = deque([data])  \n    while deque_:\n        node = deque_.popleft()\n        if isinstance(node, dict):\n            for k, v in node.items():\n                yield k\n                deque_.append(v)\n        elif isinstance(node, Iterable) and not isinstance(node, ignore_type):\n            for i in node:\n                deque_.append(i)\n</code></pre> dfs.py<pre><code>from collections.abc import Iterable\n\n\ndef get_keys(data, ignore_type=(str, bytes)):\n    if isinstance(data, dict):\n        for k, v in data.items():\n            yield k\n            yield from get_keys(v)\n    elif isinstance(data, Iterable) and not isinstance(data, ignore_type):\n        for i in data:\n            yield from get_keys(i)\n</code></pre>"},{"location":"blog/2024/01/17/capturing-grep-real-time-output/","title":"Capturing Grep Real-Time Output","text":"<p>Using <code>subprocess.Popen</code> to execute persistent commands and parse the standard output in real-time is a common practice, for example:</p> shell=Falseshell=True <pre><code>import subprocess\n\n\nping_popen = subprocess.Popen(['ping', 'lanbaoshen.github.io'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n</code></pre> <pre><code>import subprocess\n\n\nping_popen = subprocess.Popen('ping lanbaoshen.github.io', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n</code></pre> <p>Sometimes, when you already know and want to filter the keywords included in the standard output,  you will concatenate the <code>grep</code> command instead of using <code>str</code> or <code>bytes</code> function via Python:</p> shell=Falseshell=True <pre><code>import subprocess\n\n\nping_popen = subprocess.Popen(['ping', 'lanbaoshen.github.io'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\ngrep_popen = subprocess.Popen(['grep', 'time'], stdin=ping_popen.stdout, stdout=subprocess.PIPE)\n</code></pre> <pre><code>import subprocess\n\n\ngrep_popen = subprocess.Popen('ping lanbaoshen.github.io | grep time', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n</code></pre> <p>However, it will only print all the output within period at certain intervals, especially for commands like <code>ping</code>.  The output is printed out approximately every 5 minutes.</p> <p>It seems like there's a buffer in somewhere quietly affecting the output print by certain interval instead of real-time.</p> <p>Check command <code>grep</code> options with:</p> man grep--line-buffered    Force output to be line buffered. By default,     output is line buffered when standard output     is a terminal and block buffered otherwise. <p>Get it, the <code>grep</code> uses block buffering when standard output is not a terminal. </p> <p>If you want to achieve real-time effects, you need to specify <code>--line-buffered</code> to set line buffering instead of block buffering:</p> shell=Falseshell=True <pre><code>import subprocess\n\n\nping_popen = subprocess.Popen(['ping', 'lanbaoshen.github.io'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\ngrep_popen = subprocess.Popen(['grep', '--line-buffered', 'time'], stdin=ping_popen.stdout, stdout=subprocess.PIPE)\n</code></pre> <pre><code>import subprocess\n\n\ngrep_popen = subprocess.Popen('ping lanbaoshen.github.io | grep --line-buffered time', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n</code></pre> <p>Warning</p> <p>Using line buffering can cause a performance penalty. </p>"},{"location":"blog/2024/02/25/circular-import/","title":"Circular Import","text":"<p>In this document I will introduce how circular import commonly arise and what are the main ways to fix them.</p>"},{"location":"blog/2024/02/25/circular-import/#modules-dependency","title":"Modules Dependency","text":""},{"location":"blog/2024/02/25/circular-import/#cause","title":"Cause","text":"main.py<pre><code>import module_a\n</code></pre> module_a.py<pre><code>from module_b import func_b\n\n\ndef func_a():\n    func_b()\n</code></pre> module_b.py<pre><code>from module_a import func_a\n\n\ndef func_b():\n    func_a()\n</code></pre> <p>At runtime, <code>main</code> tried to import something from <code>module_a</code>, so it start to initialize <code>module_a</code>.  It starts to running the code from <code>module_a</code>, but the first line in <code>module_a</code> is to import something from <code>module_b</code>,  so it stops initializing <code>module_a</code>, because <code>module_b</code> has to be initialized first.</p> <p>And then, it hops over to <code>module_b</code> and starts to running that code instead.  But then the first line in <code>module_b</code> is that it needs something from <code>module_a</code>,  so it stops running <code>module_b</code> and goes over to <code>module_a</code> and it would like to just start initializing <code>module_a</code>.</p> <p>However, it realizes that <code>module_a</code> is already in the process of initialization, so that's where the error occurs </p>"},{"location":"blog/2024/02/25/circular-import/#solution","title":"Solution","text":"<p>We don't actually use <code>func_a</code> and <code>func_b</code> at import time, there is no true cyclic dependency.</p> <p>If <code>func_a</code> is the only place that you need <code>func_b</code>, you can take the import and put it inside <code>func_a</code>:</p> module_a.py<pre><code>def func_a():\n    from module_b import func_b\n    func_b()\n</code></pre> <p>This could actually be even more efficient, because if you never call <code>func_a</code>,  then it may be that you never even have to import <code>module_b</code> at all.</p> <p>But if you have a batch of functions in <code>module_a</code> and a lot of different ones use the <code>func_b</code>.  In that case, the better solution would be to just import the <code>module_b</code> directly:</p> module_a.py<pre><code>import module_b\n\n\ndef func_a():\n    module_b.func_b()\n</code></pre> module_b.py<pre><code>import module_a\n\n\ndef func_b():\n    module_a.func_a()\n</code></pre> <p>First, <code>main</code> tries to import <code>module_a</code> cause <code>module_a</code> to start running.  In <code>module_a</code> we get to the import <code>module_b</code> which triggers <code>module_b</code> to start running.</p> <p>In <code>module_b</code> when we get to the import <code>module_a</code>, because the <code>module_a</code> has already started initializing,  this module object technically exists. </p> <p>Since it exists, it doesn't start running this again.  So the <code>module_b</code> will just continue on finished it import and then after that import is done,  the <code>module_a</code> will finish importing.</p>"},{"location":"blog/2024/02/25/circular-import/#type-hint","title":"Type Hint","text":""},{"location":"blog/2024/02/25/circular-import/#cause_1","title":"Cause","text":"main.py<pre><code>import module_a\n</code></pre> module_a.py<pre><code>from module_b import B\n\n\nclass A:\n    def __init__(self, b: B):\n        self.b = b\n</code></pre> module_b.py<pre><code>from module_a import A\n\n\nclass B:\n    def __init__(self, a: A):\n        self.a = a\n</code></pre> <p>Type annotations are defined on class definition, when the class is defined, not when it is run.</p>"},{"location":"blog/2024/02/25/circular-import/#solution_1","title":"Solution","text":"<p>If all we care about is static analysis, then we don't even need to do the import at runtime:</p> module_a.py<pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from module_b import B\n\n\nclass A:\n    def __init__(self, b: B):\n        self.b = b\n</code></pre> module_b.py<pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from module_a import A\n\n\nclass B:\n    def __init__(self, a: A):\n        self.a = a\n</code></pre> <p>If you put all the import that you just need for the purpose of type-checking,  then none of those will actually happen at runtime, completely avoiding the import loop.</p> <p>The <code>from __future__ import annotations</code> changes the way that annotation work,  instead of evaluating <code>A</code> and <code>B</code> as a name, all annotations are converted to string. Even though <code>A</code> and <code>B</code> are not imported, the string <code>A</code> and <code>B</code> certainly exist.</p>"},{"location":"blog/2024/02/25/circular-import/#subpackage-init-cycle","title":"Subpackage Init Cycle","text":""},{"location":"blog/2024/02/25/circular-import/#cause_2","title":"Cause","text":"main.py<pre><code>from pkg.subpkg_b.module_b import B\n</code></pre> pkg/subpkg_a/__init__.py<pre><code>from .module_a import A\nfrom .module_c import C\n</code></pre> pkg/subpkg_a/module_a.py<pre><code>from ..subpkg_b.module_b import B\n\n\nclass A:\n    ...\n</code></pre> pkg/subpkg_a/module_c.py<pre><code>class C:\n    ...\n</code></pre> pkg/subpkg_b/__init__.py<pre><code>from .module_b import B\n</code></pre> pkg/subpkg_b/module_b.py<pre><code>from ..subpkg_a.module_c import C\n\n\nclass B:\n    ...\n</code></pre> <p>Our subpackage <code>init</code> files here are re-exporting some names in this case classes from some of the modules inside of them. The problem is that every module in a subpackage depends on the <code>init</code> of the subpackage. </p> <p>So if the <code>init</code> of the subpackage depends on all the modules in the subpackage,  then we have kind of made a fake dependency of every module in the subpackage on every other module in the subpackage.</p> <p>That means even though <code>module_c</code> has no import at all, <code>module_c</code> now automatically also depends on <code>module_a</code> because it depends on the <code>init</code>.</p> <p><code>main</code> tries to import something from <code>module_b</code>, we first have to run the subpackage <code>init</code>, all <code>subpkg_b</code> does is import from <code>module_b</code>, so we start to import <code>module_b</code>.</p> <p><code>module_b</code> wants to import something from <code>module_c</code>, so we first have to initialize <code>subpkg_a</code>.  But <code>subpkg_a</code> import from <code>module_a</code>, and <code>module_a</code> import from <code>module_b</code>, so now we have a cycle,  because we haven't finished initializing <code>module_b</code>.</p>"},{"location":"blog/2024/02/25/circular-import/#solution_2","title":"Solution","text":"<p>The way to get rid of the cycle is to just make all of your <code>init</code> files blank.</p> <p>If you still want to give your users a short way to import names, you can define an interface package:</p> pkg/interface_a/__init__.py<pre><code>from ..subpkg_a.module_a import A\nfrom ..subpkg_a.module_c import C\nfrom ..subpkg_b.module_b import B\n</code></pre> main.py<pre><code>from pkg.interface_a import A, B, C\n</code></pre>"},{"location":"blog/2024/06/06/enhance-zsh/","title":"Enhance Zsh","text":"<p>On macOS, the default shell is zsh. There are many ways to enhance zsh to optimize our experience.</p> <p>If you are using Ubuntu, then you first need to install zsh:</p> <pre><code>sudo apt install zsh -y\nchsh -s /bin/zsh\n</code></pre>"},{"location":"blog/2024/06/06/enhance-zsh/#oh-my-zsh","title":"Oh My Zsh","text":"<p>First of all, install Oh My Zsh:</p> <pre><code>sh -c \"$(curl -fsSL https://install.ohmyz.sh/)\"\n</code></pre> <p>Show command execution time in history command output:</p> ~/.zshrc<pre><code>...\n\n# Uncomment the following line if you want to change the command execution time\n# stamp shown in the history command output.\n# You can set one of the optional three formats:\n# \"mm/dd/yyyy\"|\"dd.mm.yyyy\"|\"yyyy-mm-dd\"\n# or set a custom format using the strftime function format specifications,\n# see 'man strftime' for details.\nHIST_STAMPS=\"yyyy-mm-dd\"\n\n...\n</code></pre> <p>Enable plugins:</p> ~/.zshrc<pre><code>...\n\n# Which plugins would you like to load?\n# Standard plugins can be found in $ZSH/plugins/\n# Custom plugins may be added to $ZSH_CUSTOM/plugins/\n# Example format: plugins=(rails git textmate ruby lighthouse)\n# Add wisely, as too many plugins slow down shell startup.\nplugins=(git copypath copyfile)\n\n...\n</code></pre>"},{"location":"blog/2024/06/06/enhance-zsh/#zsh-users","title":"zsh-users","text":"<p>Zsh community projects with zsh plugins, suggestions: zsh-syntax-highlighting,  zsh-autosuggestions,  zsh-history-substring-search</p> <pre><code>git clone https://github.com/zsh-users/zsh-syntax-highlighting ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\ngit clone https://github.com/zsh-users/zsh-history-substring-search ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-history-substring-search\n</code></pre> ~/.zshrc<pre><code>...\n\n# Which plugins would you like to load?\n# Standard plugins can be found in $ZSH/plugins/\n# Custom plugins may be added to $ZSH_CUSTOM/plugins/\n# Example format: plugins=(rails git textmate ruby lighthouse)\n# Add wisely, as too many plugins slow down shell startup.\nplugins=(... zsh-syntax-highlighting zsh-autosuggestions zsh-history-substring-search)\n\n...\n</code></pre> <p>Note</p> <p>You can configure the shortcut key of zsh-history-substring-search:</p> ~/.zshrc<pre><code>... \n\n# History-substring-search\nbindkey \"$terminfo[kcuu1]\" history-substring-search-up\nbindkey \"$terminfo[kcud1]\" history-substring-search-down\n</code></pre>"},{"location":"blog/2025/06/20/my-ai-product-%E2%91%A0/","title":"My AI Product \u2460","text":"<p>In October 2024, under the wave of AI,  my main development work shifted from the automation testing framework to the development of AI products. At this point, I only use AI as a search engine.</p> <p>Base on my experience with the automation testing framework,  the first AI product I want to develop is an AI Code generator, it primarily focuses on the generation of automated test scripts.  And I named it Step-Copilot.  </p>"},{"location":"blog/2025/06/20/my-ai-product-%E2%91%A0/#implementation","title":"Implementation","text":"<p>In our workflow, we manage the test cases on Jira via Xray plugins,  and I create a script to automatically generate test scripts template which contain the  test action and expected result based on the test cases in Jira.</p> <p>Compared to widely known code assistants like GitHub Copilot.  I only expect Step-Copilot to be able to generate the implementation code for action and expected result  base on the code which tester has written in the past. </p> <p>So, I write a simple script to export the test scripts from our repo via AST, and format them into a JSON file. The structure of the JSON file is like this:</p> <pre><code>[\n    {\n        \"input\": \"&lt;action&gt;Test Action&lt;/action&gt;&lt;expected&gt;Test Expected Result&lt;/expected&gt;\",\n        \"output\": \"The code for the action and expected result\"\n    }\n]\n</code></pre> <p>With the help of other professional AI engineers  (Without them, I think it would be very difficult for me to get started with AI application development.), I use the above JSON file to fine-tune a pre-trained LLM model and integrate it as a service with my script using FastAPI.</p>"},{"location":"blog/2025/06/20/my-ai-product-%E2%91%A0/#issues","title":"Issues","text":"<p>This model performs exceptionally well in generating non-UI operation code.  For example, we need to use the <code>play_voice</code> method to play a voice in the test script, Step-Copilot can effectively set different parameters based on different actions.</p> <p>However, when generate UI operation code, it often hallucinates and calls non-existent methods. Although I have many ideas to improve it, I still stopped the development of Step-Copilot at this point.</p>"},{"location":"blog/2025/06/20/my-ai-product-%E2%91%A0/#reflection","title":"Reflection","text":"<p>The entire product development, testing, and user feedback collection took approximately 15 working days. During this process, I divided the code generation into three levels of difficulty:</p> <ol> <li>Very Easy: write a bubble sort algorithm, a fixed function where only the parameters change  (and the accepted parameters do not include complex objects). Or some fixed, repetitive code.  These are pieces of code that people can instantly verify for correctness.</li> <li>Moderate: Code that both humans and AI can generate,  but the AI's output is difficult to verify at a glance\u2014requiring more time to validate than writing it manually.</li> <li>Complex: Complex or difficult code, or when one lacks the knowledge base. For example:  Asking a Python engineer to develop an Android application.</li> </ol> <p>Writing the UI test scripts is a moderate level task, rather than waiting for AI to complete the script and then debug it, it's better to just develop it directly. So, I stopped it.</p> <p>In my opinion, this was a failed project, but fortunately,  it didn't take much time, and through it, I gained some understanding of AI.</p>"},{"location":"blog/2025/06/21/my-ai-product-%E2%91%A1/","title":"My AI Product \u2461","text":"<p>In the previous post, I introduced <code>Step-Copilot</code>, and I stop the development of it at November 2024. Then, I thought how great it would be if AI could call the tools we had developed? </p> <p>We had previously built many tools, such as batch operations for Jira, Jenkins maintenance, and others,  which provided services to users through GUIs or web interfaces.  But regardless of the form, there is always some learning cost involved.</p> <p>Even with comprehensive documentation provided,  most users would likely ignore it and choose to directly ask the tool developers instead.</p> <p>It would be incredibly helpful to have a AI that not only uses all the tools  but also teaches users which scenarios to apply each tool in.</p> <p>Based on this idea, I developed the JAA</p>"},{"location":"blog/2025/06/21/my-ai-product-%E2%91%A1/#implementation","title":"Implementation","text":"<p>JAA is based on AutoGen, a Microsoft open-source multi-agent framework.</p> <p>AutoGen make it easy to build AI agents, and it can easily register tools for the Agent, just like the following code:</p> <pre><code>from autogen_agentchat.agents import AssistantAgent\n\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny with a high of 25\u00b0C.\"\n\nagent = AssistantAgent(\n        name='weather_agent',\n        model_client=replace_with_your_model_client,\n        # reflect_on_tool_use=True,  # If the tool's response is not in natural language, the AI will organize the returned data into a structured format.\n        tools=[get_weather],\n)\n</code></pre> <p>I registered a series of tools for JAA via python lib <code>atlassian-python-api</code>. Note, the methods provided by <code>atlassian-python-api</code> cannot be directly used by the Agent;  annotations and parameter definitions need to be supplemented.</p> <p>I also specifically declared a tool for generating ECharts charts: <pre><code>def save_echarts_chart(html: str) -&gt; str:\n    \"\"\"\n    Save echarts chart to file and return the url.\n\n    Args:\n        html: Echarts html content\n\n    Returns:\n        Url of the saved chart\n    \"\"\"\n    html_file = 'xxxx'\n    with open(html_file, 'w') as f:\n        f.write(html)\n    return html_file\n</code></pre></p> <p>With the tool in place, JAA allows users to organize complex Jira operations through natural language, for example:</p> <p>User say: <code>Obtaining the status field of all issues in the active sprint of the Board 'XXX' and displaying it with a Pie Chart.</code></p> <p>JAA will complete the task by following these steps:</p> <ol> <li>Search the board by user input name, and get the board ID.  Since the input includes \"active sprint\" the agent will select the Agile board when multiple boards are queried.</li> <li>Get the active sprint ID of the board.</li> <li>Query the target issues fields via JQL tool.</li> <li>Generate the ECharts chart HTML code based on the issues data.</li> <li>Return the result to the user.</li> </ol>"},{"location":"blog/2025/06/21/my-ai-product-%E2%91%A1/#challenges","title":"Challenges","text":"<p>The process may seem simple, but in reality, JAA took me nearly two months to complete. </p> <p>Apart from developing the frontend to provide a better user experience  (which relied heavily on AI, in the early stages, to avoid frontend development, I built it as a command-line application.), most of my time was spent on tool development and optimization. </p> <p>That's right\u2014it was not just about registering tools for the Agent and calling it a day.</p> <p>During the initial launch phase, the most common issue I encountered was exceeding the token limit. </p> <p>Since each API returned a large amount of data, I had to retain only the most critical fields myself.  For example, in Jira, a user's information includes not just the <code>display name</code> and <code>short name</code> but also a URL for their avatar. Clearly, the URL was practically useless, so it had to be removed before the tool returned the data to the AI.</p> <p>After spending a significant amount of time optimizing the tools, JAA can now complete user tasks faster and more accurately, while also supporting more context (operations) in a single session.</p> <p>Another challenge is that users often expect a single sentence to trigger an entire business process.  If you only provide the most basic tools (APIs), the time spent crafting prompts can leave users feeling hopeless.</p> <p>To address this, I encapsulate business process tools for power users, allowing them to choose whether to load these on the web page. </p> <p>Additionally, I provide a template feature, enabling users to save their own prompt templates or directly use the ones I\u2019ve prepared for them, eliminating the tedious copy-paste routine.</p> <p>For a long time after the launch, I made it a habit to spend time every day reviewing what users asked and how JAA responded.  I would then meet with users in person to discuss how to optimize it or identify any additional features they needed.</p>"},{"location":"blog/2025/06/21/my-ai-product-%E2%91%A1/#other","title":"Other","text":"<p>Finally, here\u2019s an idea I think works really well: I registered two tools, <code>Report Requirement</code> and <code>Report Bug</code>.  Users can simply tell the Agent: \"Submit a bug/requirement to the author\" and it will organize the content based on our conversation.  This makes it incredibly convenient for users to provide feedback to me.</p>"},{"location":"blog/2025/07/01/my-ai-product-%E2%91%A2/","title":"My AI Product \u2462","text":"<p>The previously mentioned JAA has been operating continuously for six months and has gained a stable user base.  Tomorrow, I need to attend the company's OPEN AI DAY to share about JAA in Shanghai.</p> <p>If it were half a year ago, I would have been very willing and had a lot to say,  but now, I think it's already somewhat outdated.</p> <p>After JAA had been running for a short while, I once thought about what other directions it could develop in:</p> <p>First, the core of JAA is function call, so simply replacing the tools can turn it into a Jenkins AI Assistant, GitHub AI Assistant, and so on.</p> <p>Second, perhaps I can package the tool part of my code into a Python library.  Then others can download this library and develop applications that go beyond my existing framework (I'm using AutoGen).</p> <p>While I was having these ideas (early December 2024), Anthropic introduced MCP at the end of the previous month.  So I immediately went to GitHub to create a repo named <code>mcp-jira</code>,  but I discovered that mcp-atlassian (Jira + Confluence) already existed and had 100 stars. Instead, I contributed some code to that repository and discussed a few ideas with the author via GitHub.</p> <p>At the same time, since I also needed to use Jenkins for work,  I developed another project called mcp-jenkins. Although there were other similar repo at the time, they had few stars, limited tool support,  and I felt their engineering practices were lacking.</p> <p>After developing my own MCP server, I abandoned the initial idea of replacing function calls with MCP.</p> <p>I realized that MCP is better suited for providing general-purpose functionalities,  while my JAA contains a lot of independent business logic that might change with API versions.  From both a security and practicality perspective, it was not suitable to serve as an MCP server.</p>"},{"location":"blog/2025/07/17/my-ai-product-%E2%91%A3/","title":"My AI Product \u2463","text":"<p>In the entire automated testing process, the most time-consuming tasks are maintaining UI changes and analyzing test reports. So, I want AI to analyze the test results to assist testers in improving the efficiency of report analysis.</p> <p>This project is named RAA, base on AutoGen.</p>"},{"location":"blog/2025/07/17/my-ai-product-%E2%91%A3/#measurement","title":"Measurement","text":"<p>I believe that confirming the metrics is the top priority before starting development. </p> <p>The most straightforward aspect is how to reflect efficiency improvements.  Directly asking developers whether time was saved is the ugliest approach, especially when perceived efficiency improves but actually takes more time.</p> <p>Since our automated testing infrastructure is well-established with a dedicated platform where report analysis is completed directly,  I can use tracking points to measure how long it takes for developers from starting the analysis to sending the report.</p> <p>Warning</p> <p>The quality of AI performance and efficiency improvements are not directly linearly correlated.</p> <p>We also need additional metrics to evaluate the performance of AI analysis,  which will guide future optimization efforts. </p> <p>In my case, the analysis conclusion of a failed case must include the category,  issue link (we use Jira for management), and a comment of the issue.</p> <p>After the dev sends the report, the actual values from the user report and the AI's values will be compared to calculate a similarity rate. Based on this similarity rate, I can identify the areas where the AI's performance is lacking.</p>"},{"location":"blog/2025/07/17/my-ai-product-%E2%91%A3/#design","title":"Design","text":"<p>It is necessary to clarify what kind of data analysis the AI should be based on and how to provide the data to the AI: 1. Screenshot of the device when it fails. 2. By combining the stack trace at the time of failure and the information from the Jira case,  determine what needs to be done, how to do it, and what errors occurred during execution. 3. Historical analysis results can help the AI understand relevant defects and learn user preferences. 4. Developers can customize rules and analysis preferences for the AI.</p> <p>For security considerations, I chose a local 7B multimodal model, MINICPM-V, for handling screenshots.  To improve the accuracy of image recognition and the validity of the results, I fine-tuned this model by annotating some data myself.</p> <p>The case information on Jira can be retrieved via the official API.  To optimize requests to other platforms, I used Redis to cache certain data.  For example, case information doesn't change frequently, so it's stored for 30 days,  while defect tickets, which require higher timeliness, are cached for 12 hours. I input the failed case script code and the actual retrieved information into my custom agent, which then summarizes: 1. What the test case was expected to do 2. How the code implemented it 3. Which steps succeeded 4. Which steps failed, and the reasons for the failures</p> <p>Historical analysis results are stored in a database.  During AI analysis, relevant results are queried based on similarity (case key + failed code line + exception message) for reference.  This is implemented using AutoGen's ChromaDB memory, with Redis as an optimization layer.  The result indices are stored in Redis and expire if unused for 15 days (or after at least two analysis cycles),  after which they are deleted from the database.</p> <p>I provide users with an entry to write their own prompts per test application.  When analyzing issues, the application fetches these prompts dynamically, which is also implemented using memory.</p>"},{"location":"blog/2025/07/17/my-ai-product-%E2%91%A3/#efficiency","title":"Efficiency","text":"<p>The project is already live. Without optimizing individual applications,  the average similarity rate fluctuates between 40% and 75%. Additionally, as the user's analysis history grows, the similarity rate shows a steady improvement.</p> <p>Based on time statistics, the approximate time saved is around 20%.</p>"},{"location":"blog/2024/06/02/remote-control-of-android-devices/","title":"Remote Control of Android Devices","text":"<p>Sometimes, we want to remotely control android devices which connected to another node like below:</p> <pre><code>graph LR\n  B[Emulator] --&gt; A[Node IP:&lt;192.168.98.87&gt;];\n  C[Android Phone] --&gt; A;\n  D[Rack] --&gt; A;\n  E[...] --&gt; A;</code></pre> <p>First of all, we need to know that each adb device has a control port. </p> <p>For example, when you run an android emulator named <code>emulator-5554</code>, you can connect to it by running <code>adb connect 127.0.0.1:5555</code>.</p> <p>So as long as we can access the port <code>192.168.98.87:5555</code>, we can remotely control <code>emulator-5554</code>.</p> <p>Nginx is a good choice to do this. To do this:</p> <p>Install nginx on the node.</p> <pre><code>sudo apt install nginx -y\n</code></pre> <p>Configure port forwarding in nginx config file:</p> /etc/nginx/nginx.conf<pre><code>stream {\n    server {\n        listen 9887;\n        proxy_pass 127.0.0.1:5555;\n    }\n}\n</code></pre> <p>Reload nginx:</p> <pre><code>sudo nginx -s reload\n</code></pre> <p>After the above steps, we can connect to the emulator by running <code>adb connect 192.168.98.87:9887</code>.  And we can use <code>scrcpy</code> to do a screencast of the emulator.</p>"},{"location":"blog/2024/12/21/running-local-model-with-ollama/","title":"Running Local Model with Ollama","text":"<p>Sometimes, I need to run a model locally on my machine. This is useful for debugging and testing purposes. </p> <p>This guide will show you how to run a model locally using Ollama.</p>"},{"location":"blog/2024/12/21/running-local-model-with-ollama/#install-ollama","title":"Install Ollama","text":"<p>We can install Ollama from web page</p> <p>If you are using linux, you can install it using the following command:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"blog/2024/12/21/running-local-model-with-ollama/#download-model","title":"Download Model","text":"<p>I will use llama3.2 model in the following example. So we need to download it first:</p> # 1B parameters, may take up 1.3GB of disk space# ollama pull llama3.2:1b# 3B parameters (default), may take up 2GB of disk spaceollama pull llama3.2success"},{"location":"blog/2024/12/21/running-local-model-with-ollama/#run-model","title":"Run Model","text":"<p>We can run the model using a command and interact with it using the terminal:</p> ollama run llama3.2helloHello! How can i assist you today?Tell me a jokeWhy don't eggs tell jokes?Because they'd crack each other up!/bye"},{"location":"blog/2024/12/21/running-local-model-with-ollama/#interact-via-api","title":"Interact via API","text":"<p>We can interact with the model using the API: <pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Tell me i joke\"\n    }\n  ],\n  \"stream\": false\n}'\n</code></pre></p>"},{"location":"blog/2024/12/21/running-local-model-with-ollama/#interact-via-code","title":"Interact via Code","text":"<p>We can interact with the model using third party lib <code>ollama</code>, we need to install it first: <pre><code>pip install -U ollama\n</code></pre></p> <p>And then we can execute the following code: <pre><code>import ollama\n\n\nresponse = ollama.chat(\n  'llama3.2',\n  messages=[{'role': 'user', 'content': 'Hello'}],\n)\nprint(response.message.content)  # How can I assist you today?\n</code></pre></p>"},{"location":"blog/2024/12/21/running-local-model-with-ollama/#example","title":"Example","text":"<p>Here is a sample example of how to use <code>ollama</code> to collect statistical data:</p> <pre><code>import ollama\n\n\ncsv_content = \"\"\"\nname, score\nMindy Luo, 97\nSovin Yang, 91\nLeo Li, 90\nLanbao Shen, 92\nChang Li, 99\nJarvan Shi, 93\n\"\"\"\n\nprompt = \"\"\"\nFind the student with the highest score and calculate the average score in the following CSV file.\nOnly return result, no need to show the process.\n\n# CSV File\n{input}\n\"\"\"\n\nresponse = ollama.chat(\n  'llama3.2',\n  messages=[{'role': 'user', 'content': prompt.format(input=csv_content)}],\n)\nprint(response.message.content)\n</code></pre> <p>When you run the code above, you may find the result is not right. This is the problem we will address later, and it will not be elaborated on here.</p> <p>Note</p> <p>As of this writing, <code>ollama</code> has supported function calls since version 0.4.0. You can refer to functions-as-tools for more information.</p>"},{"location":"blog/2024/02/20/source-code-batch-changes/","title":"Source Code Batch Changes","text":"<p>Sometimes, existing code needs to be modified in batches,  and these operations are too complex to be achieved through the IDE's replacement function.</p> <p>AST is a good choice to do that.</p> For example, perform operations on the following code: <ul> <li>Replace class name <code>Old</code> with <code>New</code> </li> <li>Replace class variable name <code>OLD</code> with <code>NEW</code> </li> <li>Replace <code>dict({key: value})</code> with <code>Base(key=value)</code></li> <li>Remove empty class</li> <li>Remove <code>main</code></li> </ul> elements.py<pre><code>class OldElements:\n    FOO = ''\n    OLD_BTN = {'text': 'Lanbao'}\n    OLD_CHECKBOX = {'resource_id': 'Lanbao'}\n    OLD_BAR = {'xpath': FOO + 'Lanbao'}\n\n\nclass FooElements(OldElements):\n    FOO = ' '\n    OLD_BTN = {'text': ''}\n\n\nclass EmptyElements:\n    ...\n\n\nif __name__ == '__main__':\n    ...\n</code></pre> <p>Visiting AST node through NodeTransformer:</p> transformer.py<pre><code>from ast import *\n\nimport astor\n\n\nclass _BaseTransformer(NodeTransformer):\n    def visit_ClassDef(self, node: ClassDef):\n        if len(node.body) == 1:\n            return None\n\n        if node.name.startswith('Old'):\n            node.name = node.name.replace('Old', 'New', 1)\n\n        for base in node.bases:\n            if base.id.startswith('Old'):\n                base.id = base.id.replace('Old', 'New', 1)\n\n        return node\n\n    def visit_If(self, node: If):\n        if node.test.left.id == '__name__':\n            return None\n\n\nclass ElementTransformer(_BaseTransformer):\n    def visit_ClassDef(self, node: ClassDef):\n        if not (node := super().visit_ClassDef(node)):\n            return None\n\n        for sub_node in node.body:\n            if not isinstance(sub_node, Assign):\n                continue\n            for target in sub_node.targets:\n                if target.id.startswith('OLD'):\n                    target.id = target.id.replace('OLD', 'NEW', 1)\n\n                if isinstance(sub_node.value, Dict):\n                    keywords = [\n                        keyword(arg=key.value, value=value)\n                        for key, value in zip(sub_node.value.keys, sub_node.value.values)\n                    ]\n\n                    sub_node.value = Call(\n                        func=Name(id='Base', ctx=Load()),\n                        args=[],\n                        keywords=keywords,\n                    )\n\n        return node\n\n\nif __name__ == '__main__':\n    tree = astor.parse_file('elements.py')\n    et = ElementTransformer()\n    modify_tree = et.visit(tree)\n    with open('new_elements.py', 'w') as file:\n        file.write(unparse(modify_tree))\n</code></pre> <p>Execute <code>transformer.py</code> to get the modified code:</p> new_elements.py<pre><code>class NewElements:\n    FOO = ''\n    NEW_BTN = Base(text='Lanbao')\n    NEW_CHECKBOX = Base(resource_id=f'{FOO}Lanbao')\n    NEW_BAR = Base(xpath=FOO + 'Lanbao')\n\nclass FooElements(NewElements):\n    FOO = ' '\n    NEW_BTN = Base(text='')\n</code></pre> <p>Warning</p> <p>Notice that <code>ast.parse</code> discards all the comments.  And if you need to format the code, you may want to explore tools or libraries like <code>autopep8</code> or <code>black</code>.</p>"},{"location":"blog/2024/02/22/stop-or-kill-a-thread/","title":"Stop or Kill a Thread","text":"<p>It is generally a bad pattern to kill a thread abruptly, in Python, and in any language.</p> <p>The nice way of handling this, if you are managing your own thread,  is to have an exit flag that each thread checks on a regular interval to see if it is time to exit:</p> <pre><code>import threading\n\n\nclass StoppableThread(threading.Thread):\n    def __init__(self,  *args, **kwargs):\n        super(StoppableThread, self).__init__(*args, **kwargs)\n        self._stop_event = threading.Event()\n\n    def stop(self):\n        self._stop_event.set()\n\n    def _stopped(self):\n        return self._stop_event.is_set()\n\n    def run(self):\n        while True:\n            if self._stopped():\n                break\n</code></pre> <p>However, when you really need to kill a thread. An example is when you are performing a long-blocking operation, and you want to interrupt it:</p> <pre><code>import ctypes\nimport threading\n\n\ndef stop_thread(t: threading.Thread, exited_ok: bool = False):\n    if any((t.is_alive(), exited_ok)) is False:\n        raise threading.ThreadError('The thread is not active')\n\n    tid, exc = ctypes.c_long(t.ident), ctypes.py_object(SystemExit)\n    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, exc)\n\n    if res == 0:\n        raise ValueError('Invalid thread id')\n    elif res != 1:\n        ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)\n        raise SystemError(\"PyThreadState_SetAsyncExc failed\")\n</code></pre>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/efficiency/","title":"Efficiency","text":""},{"location":"blog/category/automated/","title":"Automated","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/page/2/","title":"Index","text":""}]}